architecture:
  name: pet
  model:
    d_pet: 256

  training:
    finetune:
      method: lora
      config:
        alpha: 2.0
        rank: 4
    batch_size: 8
    num_epochs: 20
    num_epochs_warmup: 0
    learning_rate: 1e-4

training_set:
  systems:
    read_from: "../shared/datasets/matbench-sample/train.xyz" # file where the positions are stored
    length_unit: angstrom
  targets:
    energy:
      unit: eV
      forces: false
      stress: false

validation_set:
  systems: 
    read_from: "../shared/datasets/matbench-sample/val.xyz" # file where the positions are stored
    length_unit: angstrom
  targets:
    energy:
      unit: eV
      forces: false
      stress: false

test_set:
  systems: 
    read_from: "../shared/datasets/matbench-sample/test.xyz" # file where the positions are stored
    length_unit: angstrom
  targets:
    energy:
      unit: eV
      forces: false
      stress: false
