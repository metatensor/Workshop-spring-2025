{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the model with a LJ baseline\n",
    "\n",
    "In the previous notebook, we saw that our initial model was not very stable. One\n",
    "possible improvement would be to add a Lennard-Jones potential as a baseline,\n",
    "ensuring the model is repulsive at short distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ase.io\n",
    "\n",
    "from metatensor.torch import Labels, TensorBlock, TensorMap\n",
    "import metatensor.torch as mts\n",
    "\n",
    "from metatomic.torch import System, systems_to_torch\n",
    "from metatomic.torch import NeighborListOptions\n",
    "from metatomic.torch import ModelCapabilities, ModelMetadata, ModelOutput\n",
    "from metatomic.torch import AtomisticModel\n",
    "from metatomic.torch.ase_calculator import MetatomicCalculator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Lennard-Jones energy module\n",
    "\n",
    "The Lennard-Jones potential is a classical potential with the following functional form:\n",
    "\n",
    "$$\n",
    "E = \\sum_{ij} 4 \\varepsilon \\left[\\left(\\frac{\\sigma}{r_{ij}}\\right)^{12} - \\left(\\frac{\\sigma}{r_{ij}}\\right)^6\\right]\n",
    "$$\n",
    "\n",
    "Where the sum runs over all pairs in the system with a distance below the cutoff radius $r_c$.\n",
    "\n",
    "Using the formula above directly however comes with discontinuity issues: as the\n",
    "atoms enter and leave the cutoff, there is a jump and discontinuity in energies.\n",
    "One solution is to shift the energy to 0 at the cutoff, leaving only a small\n",
    "discontinuity in the forces.\n",
    "\n",
    "$$\n",
    "E = \\sum_{ij} 4 \\varepsilon \\left[\\left(\\frac{\\sigma}{r_{ij}}\\right)^{12} - \\left(\\frac{\\sigma}{r_{ij}}\\right)^6\\right] - 4 \\varepsilon \\left[\\left(\\frac{\\sigma}{r_c}\\right)^{12} - \\left(\\frac{\\sigma}{r_c}\\right)^6\\right]\n",
    "$$\n",
    "\n",
    "To be able to compute the Lennard-Jones energy of a system, we need a list of\n",
    "all pairs within the cutoff distance. Ideally, we want to get such list of pairs\n",
    "directly from the MD engine, which can use some tricks for a faster\n",
    "re-calculation of the list. \n",
    "\n",
    "In metatomic models, neighbor lists are encoded within them, and so we can directly request neighbor lists with a `requested_neighbors_lists()` function, and then accessing these neighbors lists in the `forward()` function.\n",
    "\n",
    "\n",
    "| ![TASK](img/clipboard.png) | Modify the code below to compute the LJ energy at the cutoff and `e_ij` in the loop over pairs |\n",
    "|----------------------------|------------------------------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LennardJones(torch.nn.Module):\n",
    "    def __init__(self, cutoff, parameters: Dict[int, Dict[str, float]]):\n",
    "        super().__init__()\n",
    "        self.cutoff = float(cutoff)\n",
    "\n",
    "        # The neighbors list request we are making:\n",
    "        self._neighbors = NeighborListOptions(cutoff=self.cutoff, full_list=False, strict=True)\n",
    "\n",
    "        self._lj_params = {}\n",
    "        for type_i, parameters_i in parameters.items():\n",
    "            sigma_i = parameters_i[\"sigma\"]\n",
    "            epsilon_i = parameters_i[\"epsilon\"]\n",
    "            self._lj_params[type_i] = {}\n",
    "\n",
    "            for type_j, parameters_j in parameters.items():\n",
    "                sigma_j = parameters_j[\"sigma\"]\n",
    "                epsilon_j = parameters_j[\"epsilon\"]\n",
    "\n",
    "                # combine parameters with Lorentz-Berthelot mixing rules\n",
    "                sigma = (sigma_i + sigma_j) / 2\n",
    "                epsilon = math.sqrt(epsilon_i * epsilon_j)\n",
    "\n",
    "                # compute the energy at the cutoff for these parameters, to remove it\n",
    "                # from the energy of a pair in forward\n",
    "\n",
    "                # TODO: update here\n",
    "                energy_at_cutoff = ...\n",
    "\n",
    "                self._lj_params[type_i][type_j] = [sigma, epsilon, energy_at_cutoff]\n",
    "\n",
    "    # expose the requested neighbor list so metatomic can find it\n",
    "    def requested_neighbor_lists(self) -> List[NeighborListOptions]:\n",
    "        return [self._neighbors]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        systems: List[System],\n",
    "        outputs: Dict[str, ModelOutput],\n",
    "        selected_atoms: Optional[Labels] = None,\n",
    "    ) -> Dict[str, TensorMap]:\n",
    "        if \"energy\" not in outputs:\n",
    "            return {}\n",
    "\n",
    "        total_energy = torch.zeros(len(systems), dtype=systems[0].positions.dtype)\n",
    "        for i_system, system in enumerate(systems):\n",
    "            # The neighbor list was computed by the MD engine and\n",
    "            # is available as a TensorBlock\n",
    "            neighbors = system.get_neighbor_list(self._neighbors)\n",
    "            # the samples of this block are\n",
    "            #     first_atom   second_atom   cell_shift_a   cell_shift_b   cell_shift_c\n",
    "            all_i = neighbors.samples.column(\"first_atom\")\n",
    "            all_j = neighbors.samples.column(\"second_atom\")\n",
    "    \n",
    "            # system also contains `positions`, `cell` and `types`, \n",
    "            # here we'll need types to get the correct LJ parameters\n",
    "            types = system.types\n",
    "    \n",
    "            # loop over all pairs\n",
    "            for i_pair, (atom_i, atom_j) in enumerate(zip(all_i, all_j)):\n",
    "                if selected_atoms is not None:\n",
    "                    # skip non-selected atoms\n",
    "                    atom_i_selected = [i_system, int(atom_i)] in selected_atoms\n",
    "                    atom_j_selected = [i_system, int(atom_j)] in selected_atoms\n",
    "                    if not atom_i_selected and not atom_j_selected:\n",
    "                        continue\n",
    "                \n",
    "                # get the parameters for the current pair of species\n",
    "                type_i = int(types[atom_i])\n",
    "                type_j = int(types[atom_j])\n",
    "                sigma, epsilon, shift = self._lj_params[type_i][type_j]\n",
    "                # get the distance between the two atoms\n",
    "                distance = neighbors.values[i_pair].reshape(3)\n",
    "    \n",
    "                # square of the distance between the atoms\n",
    "                r2 = distance.dot(distance)\n",
    "    \n",
    "                # TODO: update here\n",
    "                e_ij = ...\n",
    "    \n",
    "                total_energy[i_system] += e_ij - shift\n",
    "\n",
    "        # as previously, return a Dict of TensorMap containing the energy\n",
    "        block = TensorBlock(\n",
    "            values=total_energy.reshape(1, 1),\n",
    "            samples=Labels([\"_\"], torch.IntTensor([[0]])),\n",
    "            components=[],\n",
    "            properties=Labels([\"energy\"], torch.IntTensor([[0]])),\n",
    "        )\n",
    "        return {\n",
    "            \"energy\": TensorMap(Labels([\"energy\"], torch.tensor([[0]])), blocks=[block])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energies in eV and distances in Angstroms\n",
    "LJ_PARAMETERS = {\n",
    "    1: {\"sigma\": 2.32, \"epsilon\": 3.3104e-6},\n",
    "    6: {\"sigma\": 2.94, \"epsilon\": 2.3309e-6},\n",
    "    8: {\"sigma\": 2.66, \"epsilon\": 2.4673e-6},\n",
    "}\n",
    "LJ_CUTOFF = 6.0\n",
    "\n",
    "lj = LennardJones(cutoff=LJ_CUTOFF, parameters=LJ_PARAMETERS)\n",
    "lj = lj.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "frames = ase.io.read(\"propenol_conformers_dftb.xyz\", \":500\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a custom Lennard-Jones implementation, we can use the same  facilities to export it as a `AtomisticModule` and use it in ASE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capabilities = ModelCapabilities(\n",
    "    length_unit=\"angstrom\",\n",
    "    atomic_types=[1, 6, 8],\n",
    "    interaction_range=LJ_CUTOFF,\n",
    "    dtype=\"float64\",\n",
    "    supported_devices=[\"cpu\"],\n",
    "    outputs={\n",
    "        \"energy\": ModelOutput(\n",
    "            quantity=\"energy\",\n",
    "            unit=\"eV\",\n",
    "            per_atom=False,\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "# when using `MetatomicCalculator`, the neighbors lists are provided by ASE.\n",
    "#\n",
    "# notice that we don't need to export the model to a file to be able to use it with ASE.\n",
    "ase_calculator = MetatomicCalculator(AtomisticModel(lj, ModelMetadata(), capabilities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by checking that the code runs and produces a reasonable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the code runs fine on the first frame\n",
    "atoms = frames[0]\n",
    "atoms.calc = ase_calculator\n",
    "\n",
    "energy = atoms.get_potential_energy()\n",
    "if abs(energy - 3.05) > 0.1:\n",
    "    raise Exception(f\"the energy seems wrong: {energy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can modify our training set to remove the part accounted for by the LJ module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "forces = []\n",
    "\n",
    "# remove the LJ energies and forces from the targets\n",
    "for atoms in frames:\n",
    "    atoms.calc = ase_calculator\n",
    "    energies.append(atoms.info[\"dftb_energy_eV\"] - atoms.get_potential_energy())\n",
    "    forces.append(atoms.arrays[\"dftb_forces_eV_per_Ang\"] - atoms.get_forces())\n",
    "\n",
    "energies = np.vstack(energies)\n",
    "forces = np.vstack(forces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN, again!\n",
    "\n",
    "Let's re-define our model, and re-train it on the new targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featomic.torch import SoapPowerSpectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the same definition as in notebook 2\n",
    "\n",
    "class SOAPModel(torch.nn.Module):\n",
    "    def __init__(self, soap_parameters, atomic_types, energy_offset):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy_offset = torch.tensor(energy_offset)\n",
    "        self.atomic_types = atomic_types\n",
    "\n",
    "        self.soap_calculator = SoapPowerSpectrum(**soap_parameters)\n",
    "        self.neighbor_atom_types = Labels(\n",
    "            [\"neighbor_1_type\", \"neighbor_2_type\"], \n",
    "            torch.tensor([(i, j) for i in atomic_types for j in atomic_types if i <= j])\n",
    "        )\n",
    "\n",
    "        # Number of features produced by the SOAP calculator,\n",
    "        # i.e. size of the input of the NN\n",
    "        n_soap = (\n",
    "            (soap_parameters[\"basis\"][\"max_angular\"] + 1)\n",
    "            * (soap_parameters[\"basis\"][\"radial\"][\"max_radial\"] + 1) ** 2\n",
    "            * len(self.neighbor_atom_types)\n",
    "        )\n",
    "\n",
    "    \n",
    "        # we are using utilities from `metatensor-learn` to define the NN in a metatensor-compatible way\n",
    "        # https://docs.metatensor.org/latest/learn/reference/nn/index.html#metatensor.learn.nn.ModuleMap\n",
    "        self.soap_nn = mts.learn.nn.ModuleMap(\n",
    "            in_keys = Labels(\"_\", torch.tensor([[0]])),\n",
    "            modules = [torch.nn.Sequential(\n",
    "                # Definition of our NN: one hidden layer,\n",
    "                # SiLU activation, 128-sized latent space\n",
    "                torch.nn.Linear(\n",
    "                    in_features=n_soap, out_features=128, bias=False, dtype=torch.float64\n",
    "                ),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(\n",
    "                    in_features=128, out_features=128, bias=False, dtype=torch.float64\n",
    "                ),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(\n",
    "                    in_features=128, out_features=1, bias=True, dtype=torch.float64\n",
    "                ),\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        systems: List[System],\n",
    "        selected_atoms: Optional[Labels] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        energies = torch.zeros((len(systems), 1), dtype=torch.float64)\n",
    "        \n",
    "        soap = self.soap_calculator(systems, selected_samples=selected_atoms)\n",
    "        soap = soap.keys_to_properties(self.neighbor_atom_types)\n",
    "        soap = soap.keys_to_samples(\"center_type\")\n",
    "\n",
    "        energies_per_atom = self.soap_nn(soap)\n",
    "        energies = mts.sum_over_samples(energies_per_atom, [\"atom\", \"center_type\"])\n",
    "        energies = energies.block().values\n",
    "\n",
    "        return energies + self.energy_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOAP_PARAMETERS = {\n",
    "    \"cutoff\": {\n",
    "        \"radius\": 3.5,\n",
    "        \"smoothing\": {\n",
    "            \"type\": \"ShiftedCosine\",\n",
    "            \"width\": 0.2\n",
    "        }\n",
    "    },\n",
    "    \"density\": {\n",
    "        \"type\": \"Gaussian\",\n",
    "        \"width\": 0.3\n",
    "    },\n",
    "    \"basis\": {\n",
    "        \"type\": \"TensorProduct\",\n",
    "        \"max_angular\": 5,\n",
    "        \"radial\": {\n",
    "            \"type\": \"Gto\",\n",
    "            \"max_radial\": 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model = SOAPModel(\n",
    "    SOAP_PARAMETERS,\n",
    "    atomic_types=[1, 6, 8],\n",
    "    energy_offset=energies.mean(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems = systems_to_torch(frames, dtype=torch.float64)\n",
    "\n",
    "reference = torch.tensor(energies)\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "epoch = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure will be very similar to the previous one.\n",
    "\n",
    "| ![TASK](img/clipboard.png) | Run the training loop until the loss is below 0.1 |\n",
    "|----------------------------|---------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = epoch + 1\n",
    "\n",
    "for epoch in range(start, start + 15):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(systems)\n",
    "    loss = mse_loss(prediction, reference)\n",
    "    print(f\"loss at epoch {epoch} is\", loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss.item() > 0.1:\n",
    "    raise Exception(\n",
    "        f\"loss is still too high, please continue running the training loop\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the output against the reference (without the LJ contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_energy = model(systems)\n",
    "\n",
    "plt.scatter(energies, predicted_energy.detach().numpy())\n",
    "\n",
    "x = [np.min(energies), np.max(energies)]\n",
    "plt.plot(x, x, c=\"grey\")\n",
    "\n",
    "plt.title(\"energies\")\n",
    "plt.xlabel(\"reference\")\n",
    "plt.ylabel(\"predicted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-assemble the full model\n",
    "\n",
    "We can now assemble a new model using both the LJ contributions and the\n",
    "re-trained SOAP neural network. We'll run both here, and add their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, lj, nn_model):\n",
    "        super().__init__()\n",
    "        self.lj = lj\n",
    "        self.nn_model = nn_model\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        systems: List[System], \n",
    "        outputs: Dict[str, ModelOutput], \n",
    "        selected_atoms: Optional[Labels]\n",
    "    ) -> Dict[str, TensorMap]:\n",
    "        # check if the energy was even required\n",
    "        if \"energy\" not in outputs:\n",
    "            return {}\n",
    "        \n",
    "        # Run the LJ model\n",
    "        outputs = self.lj(systems, outputs, selected_atoms)\n",
    "        lj_energy = outputs[\"energy\"].block().values\n",
    "\n",
    "        # Run the NN model\n",
    "        nn_energy = self.nn_model(systems, selected_atoms)\n",
    "\n",
    "        energy = lj_energy + nn_energy\n",
    "\n",
    "        # create the output TensorMap\n",
    "        block = TensorBlock(\n",
    "            values=energy.reshape(-1, 1),\n",
    "            samples=Labels(\"system\", torch.arange(len(systems)).reshape(-1, 1)),\n",
    "            components=[],\n",
    "            properties=Labels(\"energy\", torch.tensor([[0]])),\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"energy\": TensorMap(keys=Labels(\"_\", torch.tensor([[0]])), blocks=[block])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, we can define the capabilities of our new model, and export it to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_output = ModelOutput(\n",
    "    quantity=\"energy\",\n",
    "    unit=\"eV\",\n",
    "    per_atom=False,\n",
    ")\n",
    "\n",
    "capabilities = ModelCapabilities(\n",
    "    length_unit=\"angstrom\",\n",
    "    # the interaction_range is the largest cutoff\n",
    "    interaction_range=max(SOAP_PARAMETERS[\"cutoff\"][\"radius\"], LJ_CUTOFF),\n",
    "    atomic_types=[1, 6, 8],\n",
    "    supported_devices=[\"cpu\"],\n",
    "    dtype = \"float64\",\n",
    "    outputs={\n",
    "        \"energy\": energy_output,\n",
    "    },\n",
    ")\n",
    "\n",
    "metadata = ModelMetadata(\n",
    "    name=\"LJ + SOAP NN model\",\n",
    "    description=\"...\",\n",
    "    authors=[],\n",
    "    references={\n",
    "        \"implementation\": [],\n",
    "        \"architecture\": [],\n",
    "        \"model\": [],\n",
    "    }\n",
    ")\n",
    "\n",
    "wrapper = ExportWrapper(lj, model).eval()\n",
    "metatomic_model = AtomisticModel(wrapper, metadata, capabilities)\n",
    "metatomic_model.save(\"propenol-model-with-lj.pt\", collect_extensions=\"extensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some MD with our new model\n",
    "\n",
    "Let's see if these changes have made a difference to the stability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase.md\n",
    "import ase.units\n",
    "\n",
    "import chemiscope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same initial setup for the simulation\n",
    "\n",
    "atoms = frames[0]\n",
    "atoms.calc = MetatomicCalculator(\"propenol-model-with-lj.pt\")\n",
    "\n",
    "integrator = ase.md.VelocityVerlet(atoms, timestep=1 * ase.units.fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = [atoms.copy()]\n",
    "\n",
    "for _ in range(200):\n",
    "    integrator.run(1)\n",
    "    trajectory.append(atoms.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(\n",
    "    trajectory, mode=\"structure\", settings={\"structure\": [{\"playbackDelay\": 50}]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fixed the issue of atoms getting way too close to one another! ðŸŽ‰\n",
    "\n",
    "Very obviously though, we still have other problems, from the fairly low \n",
    "accuracy to the molecule nowdecomposing into individual atoms. There would be\n",
    "a handful of approaches to improve on this point (see at the end of notebook\n",
    "3-ASE-md). If you have some time, feel free to go back to the model definition\n",
    "and training and improve on these points!\n",
    "\n",
    "For now, we'll see how we can take the exact same potential we just use in ASE\n",
    "and use it in a completely different simulation engine: LAMMPS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
