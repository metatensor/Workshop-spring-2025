{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interatomic Potentials with Metatensor\n",
    "\n",
    "This notebook contains a short example of how one can train a very simple machine learning model for energy and forces, \n",
    "built on top of metatensor and metatomic. \n",
    "\n",
    "> **WARNING:**\n",
    ">\n",
    "> To ensure a reasonable run time for this tutorial, the dataset size, model complexity and training procedure have been reduced to their absolute minimum.\n",
    "> If you want to train a model for your own research, you should make sure that the dataset size and model architecture are sufficient to get a stable and precise model.\n",
    "\n",
    "In general, when training a machine learning model we need the following ingredients:\n",
    "\n",
    "- a dataset, containing structures and their energy (and forces/virials or stresses). Here we\n",
    "  will use a dataset of conformers of 2-Propen-1-ol, with the energy and forces\n",
    "  computed using [DFTB](https://dftbplus.org/).\n",
    "- a model architecture, which defines how our machine learning model makes its predictions. Here\n",
    "  we'll use a perceptron based neural network trained on rotation invariants SOAP power\n",
    "  spectrum. This should be very close to the first generation of\n",
    "  Behler-Parrinello NNs.\n",
    "- an optimizer and loss function, used inside the training loop to optimize the NN\n",
    "  weights and ensure the model predictions match the DFTB calculations, at least at the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "import ase.io  # read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and extract energies and forces from ASE\n",
    "frames = ase.io.read(\"../propenol_conformers_dftb.xyz\", \":500\")\n",
    "\n",
    "energies = np.array([[f.info[\"dftb_energy_eV\"]] for f in frames])\n",
    "forces = np.vstack([f.arrays[\"dftb_forces_eV_per_Ang\"] for f in frames])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - The machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featomic.torch import SoapPowerSpectrum\n",
    "\n",
    "from metatensor.torch import Labels, TensorBlock, TensorMap\n",
    "\n",
    "import metatomic.torch as mta\n",
    "import metatensor.torch as mts\n",
    "\n",
    "from metatomic.torch import System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is customary when using PyTorch, our model will be a class inheriting from\n",
    "`torch.nn.Module`. In the `forward` function, we'll take\n",
    "`metatomic.torch.System`, compute the SOAP power spectrum for all atoms in the\n",
    "system and then send this representation through a neural network. This will\n",
    "give us per-atom energies, that we will then sum together to get the overall\n",
    "prediction.\n",
    "\n",
    "$$\n",
    "E = \\sum_i NN(\\langle \\alpha_1 \\alpha_2 n_1 n_2 l |\n",
    "\\rho_i^2 \\rangle)\n",
    "$$\n",
    "\n",
    "The same NN will be used regardless of the central atom species (this will be the\n",
    "first possible improvement of this model later!).\n",
    "\n",
    "Featomic outputs the SOAP Power Spectrum in a maximally sparse format, where\n",
    "each central species, species of the first neighbor $\\alpha_1$, and species of\n",
    "the second neighbor $\\alpha_2$ are stored separately, minimizing the memory\n",
    "usage and enabling varied treatments of different blocks. Here, we will just\n",
    "treat the neighboring species as [one-hot encodings](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics); and the central species as\n",
    "samples with the same behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOAPModel(torch.nn.Module):\n",
    "    def __init__(self, soap_parameters, atomic_types, energy_offset):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy_offset = torch.tensor(energy_offset)\n",
    "        self.atomic_types = atomic_types\n",
    "\n",
    "        self.soap_calculator = SoapPowerSpectrum(**soap_parameters)\n",
    "        self.neighbor_atom_types = Labels(\n",
    "            [\"neighbor_1_type\", \"neighbor_2_type\"], \n",
    "            torch.tensor([(i, j) for i in atomic_types for j in atomic_types if i <= j])\n",
    "        )\n",
    "\n",
    "        # Number of features produced by the SOAP calculator,\n",
    "        # i.e. size of the input of the NN\n",
    "        n_soap = (\n",
    "            (soap_parameters[\"basis\"][\"max_angular\"] + 1)\n",
    "            * (soap_parameters[\"basis\"][\"radial\"][\"max_radial\"] + 1) ** 2\n",
    "            * len(self.neighbor_atom_types)\n",
    "        )\n",
    "\n",
    "    \n",
    "        # we are using utilities from `metatensor-learn` to define the NN in a metatensor-compatible way\n",
    "        # https://docs.metatensor.org/latest/learn/reference/nn/index.html#metatensor.learn.nn.ModuleMap\n",
    "        self.soap_nn = mts.learn.nn.ModuleMap(\n",
    "            in_keys = Labels(\"_\", torch.tensor([[0]])),\n",
    "            modules = [torch.nn.Sequential(\n",
    "                # Definition of our NN: one hidden layer,\n",
    "                # SiLU activation, 128-sized latent space\n",
    "                torch.nn.Linear(\n",
    "                    in_features=n_soap, out_features=128, bias=False, dtype=torch.float64\n",
    "                ),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(\n",
    "                    in_features=128, out_features=128, bias=False, dtype=torch.float64\n",
    "                ),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Linear(\n",
    "                    in_features=128, out_features=1, bias=True, dtype=torch.float64\n",
    "                ),\n",
    "            )]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        systems: List[System],\n",
    "        selected_atoms: Optional[Labels] = None,\n",
    "    ) -> torch.Tensor:        \n",
    "        soap = self.soap_calculator(systems, selected_samples=selected_atoms)\n",
    "        soap = soap.keys_to_properties(self.neighbor_atom_types)\n",
    "        soap = soap.keys_to_samples(\"center_type\")\n",
    "\n",
    "        energies_per_atom = self.soap_nn(soap)\n",
    "        energy = mts.sum_over_samples(energies_per_atom, [\"atom\", \"center_type\"])\n",
    "        energy = energy.block().values\n",
    "\n",
    "        return energy + self.energy_offset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model!\n",
    "\n",
    "To simplify the task of the NN, we will enforce a constant energy offset\n",
    "corresponding to some arbitrary energy baseline (here, the mean energy of the\n",
    "training set). \n",
    "\n",
    "| ![TASK](../img/clipboard.png) | Go back to the class definition above, and add the energy offset to the prediction |\n",
    "|-------------------------------|------------------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOAP_PARAMETERS = {\n",
    "    \"cutoff\": {\n",
    "        \"radius\": 3.5,\n",
    "        \"smoothing\": {\n",
    "            \"type\": \"ShiftedCosine\",\n",
    "            \"width\": 0.2\n",
    "        }\n",
    "    },\n",
    "    \"density\": {\n",
    "        \"type\": \"Gaussian\",\n",
    "        \"width\": 0.3\n",
    "    },\n",
    "    \"basis\": {\n",
    "        \"type\": \"TensorProduct\",\n",
    "        \"max_angular\": 5,\n",
    "        \"radial\": {\n",
    "            \"type\": \"Gto\",\n",
    "            \"max_radial\": 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "energy_offset = energies.mean()\n",
    "model = SOAPModel(\n",
    "    SOAP_PARAMETERS,\n",
    "    atomic_types=[1, 6, 8],\n",
    "    energy_offset=energy_offset,\n",
    ")\n",
    "\n",
    "\n",
    "first_energy = model(mta.systems_to_torch(frames[:1], dtype=torch.float64))\n",
    "if torch.abs(first_energy + 290) > 10:\n",
    "    raise Exception(\n",
    "        f\"energy of the first structure is {first_energy.item()}, should be around -290. \"\n",
    "        \"Please modify the forward function above! Hint: you can use self.energy_offset\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the tools to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the inputs (systems) and expected outputs (reference_energies) of our model\n",
    "systems = mta.systems_to_torch(frames, dtype=torch.float64)\n",
    "\n",
    "reference_energies = torch.tensor(energies)\n",
    "\n",
    "# We'll need a loss function to compare the predictions to the actual outputs of the model\n",
    "# let's use the mean square error loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# the optimizer updates the weights of the model according to the gradients\n",
    "# a learning rate of 0.003 allows to learn fast enough while preventing the model\n",
    "# from jumping around in parameter space\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.003)\n",
    "epoch = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the training phase! We might have to run the loop multiple times to\n",
    "ensure we reach a high enough accuracy. As a starting point, we'll stop at a\n",
    "loss around 0.03, but feel free to come back and try to get the loss even\n",
    "lower!\n",
    "\n",
    "\n",
    "| ![TASK](../img/clipboard.png) | Run the training loop until the loss is below 0.03 |\n",
    "|----------------------------|----------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = epoch + 1\n",
    "\n",
    "for epoch in range(start, start + 100):\n",
    "    optimizer.zero_grad()  # set all parameters gradients to zero\n",
    "\n",
    "    predicted_energies = model(systems)  # run the model once\n",
    "\n",
    "    loss = mse_loss(predicted_energies, reference_energies)  # compute a loss\n",
    "    print(f\"loss at epoch {epoch} is\", loss.item())\n",
    "\n",
    "    loss.backward()  # backward propagate from the loss, updating all parameters gradients\n",
    "    optimizer.step()  # run one optimizer step, updating the parameters based on gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss.item() > 0.03:\n",
    "    raise Exception(\n",
    "        f\"loss is still too high, please continue running the training loop\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the energy prediction we are making against the reference\n",
    "values.\n",
    "\n",
    "In an actual research setting, you would also want to check the predictions your\n",
    "model is making on a validation/hold-out set of structures, to prevent your model\n",
    "from over-fitting to your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_energy = model(systems)\n",
    "\n",
    "plt.scatter(energies, predicted_energy.detach().numpy())\n",
    "\n",
    "x = [np.min(energies), np.max(energies)]\n",
    "plt.plot(x, x, c=\"grey\")\n",
    "\n",
    "plt.title(\"energies\")\n",
    "plt.xlabel(\"reference / eV\")\n",
    "plt.ylabel(\"predicted / eV\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code in featomic, metatomic and metatensor is fully integrated with the torch\n",
    "automatic differentiation framework, which allows us to compute the gradients of\n",
    "any output with respect to any input. In particular, we can use this to also\n",
    "predict the forces acting on the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to `metatomic.torch.System`, but now tracking\n",
    "# gradients with respect to positions\n",
    "systems_positions_grad = mta.systems_to_torch(frames, positions_requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# make a new prediction\n",
    "predicted_energy = model(systems_positions_grad)\n",
    "\n",
    "# extract the gradient of the prediction with backward propagation\n",
    "# using `torch.autograd.grad`\n",
    "predicted_forces = torch.autograd.grad(\n",
    "    outputs=predicted_energy,\n",
    "    inputs=[s.positions for s in systems_positions_grad],\n",
    "    grad_outputs=-torch.ones_like(predicted_energy),\n",
    "    create_graph=False,\n",
    "    retain_graph=False,\n",
    ")\n",
    "predicted_forces = torch.vstack(predicted_forces)\n",
    "\n",
    "plt.scatter(forces.flatten(), predicted_forces.detach().numpy().flatten())\n",
    "\n",
    "x = [np.min(forces.flatten()), np.max(forces.flatten())]\n",
    "plt.plot(x, x, c=\"grey\")\n",
    "\n",
    "plt.title(\"forces\")\n",
    "plt.xlabel(\"reference / eV/Å\")\n",
    "plt.ylabel(\"predicted / eV/Å\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:**\n",
    "> \n",
    "> The forces this model is producing are very bad, for many reasons: the dataset is very small, the model architecture is minimal, the model has not been trained on forces, only on energy. We will address some of these shortcomings in future tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model\n",
    "\n",
    "Now that we have a reasonable model, let's export it! We'll need to define some\n",
    "metadata about our model as well, so the MD engine knows which unit\n",
    "conversions to make and what the model can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metatomic.torch import (\n",
    "    AtomisticModel, \n",
    "    System, \n",
    "    ModelOutput, \n",
    "    ModelMetadata, \n",
    "    ModelCapabilities, \n",
    "    ModelEvaluationOptions,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a class conforming to the `AtomisticModel` API (https://docs.metatensor.org/metatomic/latest/torch/reference/models/export.html#metatomic.torch.ModelInterface). In this API, the model receives as its input a single structure and a set of options,\n",
    "including which outputs the engine needs. The model should then return these outputs in a dictionary of `TensorMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # model we are wrapping\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        systems: List[System], \n",
    "        outputs: Dict[str, ModelOutput], \n",
    "        selected_atoms: Optional[Labels]\n",
    "    ) -> Dict[str, TensorMap]:\n",
    "        # check if the energy was even required\n",
    "        if \"energy\" not in outputs:\n",
    "            return {}\n",
    "        \n",
    "        # Run the model\n",
    "        energy = self.model(systems, selected_atoms)\n",
    "\n",
    "        # Return our prediction in a Dict[str, TensorMap]. Here there isn't much\n",
    "        # metadata to attach to the output, but this will change if we are returning\n",
    "        # per-atom energy, or more complex outputs (dipole moments, electronic density,\n",
    "        # etc.)\n",
    "        block = TensorBlock(\n",
    "            values=energy.reshape(-1, 1),\n",
    "            samples=Labels(\"system\", torch.arange(len(systems)).reshape(-1, 1)),\n",
    "            components=[],\n",
    "            properties=Labels(\"energy\", torch.tensor([[0]])),\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"energy\": TensorMap(keys=Labels(\"_\", torch.tensor([[0]])), blocks=[block])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in our export wrapper\n",
    "wrapper = ExportWrapper(model)\n",
    "wrapper = wrapper.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before exporting our model is to define its capabilities: what\n",
    "this model can compute; what are the expected inputs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model has a single output: the energy\n",
    "energy_output = ModelOutput(\n",
    "    quantity=\"energy\",\n",
    "    # energy is returned in eV\n",
    "    unit=\"eV\",\n",
    "    # energy is returned globally, not per-atom\n",
    "    per_atom=False,\n",
    ")\n",
    "\n",
    "# overall capabilities of the model\n",
    "capabilities = ModelCapabilities(\n",
    "    # expected unit for the positions and cell vectors\n",
    "    length_unit=\"angstrom\",\n",
    "    # how far atoms are interacting together in this model\n",
    "    interaction_range=SOAP_PARAMETERS[\"cutoff\"][\"radius\"],\n",
    "    # which atomic types can this model work with\n",
    "    atomic_types=[1, 6, 8],\n",
    "    # which torch devices this model can handle\n",
    "    supported_devices=[\"cpu\"],\n",
    "    # which torch dtype is used for inputs and outputs\n",
    "    dtype = \"float64\",\n",
    "    # the outputs this model supports\n",
    "    outputs={\n",
    "        \"energy\": energy_output,\n",
    "    },\n",
    ")\n",
    "\n",
    "metadata = ModelMetadata(\n",
    "    name=\"A simple SOAP NN model\",\n",
    "    description=\"...\",\n",
    "    authors=[\"John Doe\"],\n",
    "    references={\n",
    "        \"implementation\": [],\n",
    "        \"architecture\": [],\n",
    "        \"model\": [],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ![TASK](../img/clipboard.png) | Define the atomic types this model can handle in the capabilities above, and add your name in the authors list |\n",
    "|----------------------------|-------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(capabilities.atomic_types) == 0:\n",
    "    raise Exception(\"missing atomic types in the capabilities\")\n",
    "\n",
    "if len(metadata.authors) == 0 or (len(metadata.authors) == 1 and metadata.authors[0] == \"you\"):\n",
    "    raise Exception(\"please add your name to the authors list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export our model and it's capabilities as a new\n",
    "`MetatensorAtomisticModule`, which will run a couple of checks on the model and\n",
    "handle all the units conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metatensor_model = AtomisticModel(wrapper, metadata, capabilities)\n",
    "metatensor_model.save(\"propenol-model.pt\", collect_extensions=\"extensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go to the next notebook, and run some Molecular Dynamics with our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
